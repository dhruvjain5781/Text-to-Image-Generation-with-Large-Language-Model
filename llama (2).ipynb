{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6e32c4f-1d51-45bf-891d-e1a0bb492984",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import re\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from datasets import Dataset\n",
    "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "import clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2fda70a0-13bc-460e-b355-e4f4780e6d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"starvector/text2svg-stack\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8484be46-1e95-4198-8d3f-d0a62146fcb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_all = dataset[\"train\"].to_pandas()\n",
    "test_df_all = dataset[\"test\"].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70c6b0b0-c01c-4d85-b4c1-d6e1fad0a75d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2169710\n",
      "5709\n"
     ]
    }
   ],
   "source": [
    "print(len(train_df_all))\n",
    "print(len(test_df_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8cf1592-2168-44de-932c-3f4a3f1e1fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_svg_df(df, top_n):\n",
    "    # width, height ÌïÑÌÑ∞ÎßÅ\n",
    "    def extract_svg_dims(svg_str):\n",
    "        match = re.search(r'<svg[^>]*width=\"(\\d+)\"[^>]*height=\"(\\d+)\"', svg_str)\n",
    "        if match:\n",
    "            return int(match.group(1)), int(match.group(2))\n",
    "        return None, None\n",
    "\n",
    "    df[\"svg_dims\"] = df[\"Svg\"].apply(extract_svg_dims)\n",
    "    df_filtered = df[df[\"svg_dims\"].apply(\n",
    "        lambda dims: dims[0] is not None and 800 <= dims[0] <= 1000 and 800 <= dims[1] <= 1000\n",
    "    )].copy()\n",
    "\n",
    "    # SVG Í∏∏Ïù¥ Í∏∞Ï§Ä ÏÉÅÏúÑ NÍ∞ú ÏÑ†ÌÉù\n",
    "    df_filtered[\"svg_length\"] = df_filtered[\"Svg\"].apply(len)\n",
    "    df_sorted = df_filtered.sort_values(\"svg_length\", ascending=False).head(top_n).copy()\n",
    "\n",
    "    # ÌÖçÏä§Ìä∏/SVG Ï∂îÏ∂ú\n",
    "    input_texts = df_sorted[\"caption_llava\"].astype(str).tolist()\n",
    "    target_svgs = df_sorted[\"Svg\"].astype(str).tolist()\n",
    "\n",
    "    return input_texts, target_svgs, df_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3841510b-b5a0-4adb-9957-a4a37c96cbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_train, target_train, df_train = preprocess_svg_df(train_df_all, top_n = 10000)\n",
    "input_test, target_test, df_test = preprocess_svg_df(test_df_all, top_n = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f552374c-ff7a-4e67-bb13-39b55a42e14a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "print(len(df_train))\n",
    "print(len(df_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e18f771-51ff-47ad-94d1-5b67d8541dce",
   "metadata": {},
   "source": [
    "ÏñëÏûêÌôî(Quantization)\n",
    "\n",
    "LLaMA 2 7BÎäî ÏóÑÏ≤≠ ÌÅ∞ Î™®Îç∏Ïù¥Îùº ÏùºÎ∞òÏ†ÅÏù∏ GPU (Ïòà: 12GB~24GB VRAM)ÏóêÏÑúÎäî **ÌíÄ Ï†ïÎ∞ÄÎèÑ(float32)**ÎÇò **Î∞ò Ï†ïÎ∞ÄÎèÑ(float16)**Î°úÎäî Î°úÎìúÏ°∞Ï∞® ÏïàÎê®\n",
    "\n",
    "4bit ÏñëÏûêÌôîÎ•º ÌÜµÌï¥ÏÑú : Î™®Îç∏ ÏÇ¨Ïù¥Ï¶àÎ•º ÌÅ¨Í≤å Ï§ÑÏù¥Í≥† (4Î∞∞ Ïù¥ÏÉÅ ÏûëÍ≤å) VRAM ÏÇ¨Ïö©ÎüâÎèÑ Ï§ÑÏûÑ / ÏÑ±Îä•ÏùÄ ÏµúÎåÄÌïú Ïú†ÏßÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585cb4b5-8ab6-4dde-b6e2-f3c7807a66fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "142fd1ec71dd49408c98df11964bb798",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "hf_token = \"\"\n",
    "\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    llm_4bit_enable_fp32_cpu_offload=True  # CPU Ïò§ÌîÑÎ°úÎìú ÌóàÏö©\n",
    ")\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, token=hf_token)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",  # GPUÏóê ÏûêÎèô Î∂ÑÏÇ∞ Î°úÎî©\n",
    "    quantization_config=quant_config,\n",
    "    token=hf_token\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c074023a-86e4-4e65-b082-4ffeb55b6fc6",
   "metadata": {},
   "source": [
    "PEFT(Parameter-Efficient Fine-Tuning) : Î™®Îç∏ Ï†ÑÏ≤¥Î•º Îã§ ÌïôÏäµÌïòÏßÄ ÏïäÍ≥†, ÏùºÎ∂ÄÎßå ÌïôÏäµÌï¥ÏÑú ÏÑ±Îä•ÏùÑ Ïú†ÏßÄÌïòÎ©¥ÏÑúÎèÑ ÌïôÏäµ ÎπÑÏö©ÏùÑ ÎÇÆÏ∂îÎäî Î∞©Î≤ï / Í∏∞Ï°¥ LLMÏùÄ ÏàòÏã≠Ïñµ Í∞ú ÌååÎùºÎØ∏ÌÑ∞ ‚Üí Ï†ÑÏ≤¥ ÌååÎùºÎØ∏ÌÑ∞ ÌïôÏäµÏùÄ Îß§Ïö∞ ÎπÑÏåà / PEFTÎäî ÏùºÎ∂Ä ÌååÎùºÎØ∏ÌÑ∞Îßå ÌïôÏäµÌï¥ÏÑú ÏÑ±Îä•ÏùÄ ÎπÑÏä∑ÌïòÍ≤å, ÏûêÏõêÏùÄ Ï†ÅÍ≤å -> ÎåÄÌëúÏ†ÅÏù∏ PEFT Î∞©Ïãù Ï§ë ÌïòÎÇòÍ∞Ä Î∞îÎ°ú LoRA\n",
    "\n",
    "\n",
    "LoRA(Low-Rank Adaptation) : Í∏∞Ï°¥ Î™®Îç∏ ÌååÎùºÎØ∏ÌÑ∞Î•º Í≥†Ï†ïÌïú Ï±Ñ,\"ÏûëÏùÄ Ï∂îÍ∞Ä Î†àÏù¥Ïñ¥Îßå ÌïôÏäµ\"Ìï¥ÏÑú fine-tuning ÌïòÎäî Î∞©Î≤ï"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8585d392-649a-4faa-9a47-caafb7a33c18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ñ∂ Trainable / Total = 4,194,304 / 3,504,607,232\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "\n",
    "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total     = sum(p.numel() for p in model.parameters())\n",
    "print(f\"‚ñ∂ Trainable / Total = {trainable:,} / {total:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18a00b72-9806-41d2-bfca-a61f80267ef5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "436bf13ac6424c08bf66407a17891930",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/30 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2263d992acf74937b8ad797b876a4400",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/30 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Hugging Face Dataset Íµ¨ÏÑ±\n",
    "train_dataset = Dataset.from_dict({\n",
    "    \"input\": input_train,\n",
    "    \"output\": target_train\n",
    "})\n",
    "\n",
    "# ÌîÑÎ°¨ÌîÑÌä∏ Ìè¨Îß∑\n",
    "def format_chat_prompt(example):\n",
    "    return f\"\"\"Given the <CAPTION>, generate the corresponding svg code.\n",
    "Return a response WITH SVG CODE that appropriately alignes with the request, IT SHOULD BE VERY ELABORATE.\n",
    "Before answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\n",
    "\n",
    "Please ensure that the generated SVG code is well-formed, valid, and strictly adheres to these constraints.\n",
    "Focus on a clear and concise representation of the input description within the given limitations. Always give the complete SVG code with nothing omitted. Never use an ellipsis.\n",
    "\n",
    "<constraints>\n",
    "* **Allowed Elements:** `svg`, `path`, `circle`, `rect`, `ellipse`, `line`, `polyline`, `polygon`, `g`, `linearGradient`, `radialGradient`, `stop`, `defs`\n",
    "* **Allowed Attributes:** `viewBox`, `width`, `height`, `fill`, `stroke`, `stroke-width`, `d`, `cx`, `cy`, `r`, `x`, `y`, `rx`, `ry`, `x1`, `y1`, `x2`, `y2`, `points`, `transform`, `opacity`\n",
    "</constraints>\n",
    "\n",
    "<CAPTION>\n",
    "{example['input']}\n",
    "</CAPTION>\n",
    "\n",
    "SVG:{example['output']}\"\"\"\n",
    "\n",
    "\n",
    "train_dataset = train_dataset.map(lambda x: {\"text\": format_chat_prompt(x)})\n",
    "\n",
    "# ÌÜ†ÌÅ¨ÎÇòÏù¥Ï¶à\n",
    "def tokenize(example):\n",
    "    return tokenizer(\n",
    "        example[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=512\n",
    "    )\n",
    "\n",
    "tokenized_dataset = train_dataset.map(tokenize, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca23fe5f-7eab-44ac-b332-bdae2c28bc75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c75093054a24b4695482a933e6bfbc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/30 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 0, Loss: 0.8743\n",
      "Epoch 1 Completed. Avg Loss: 0.8414\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import default_data_collator\n",
    "\n",
    "dataloader = DataLoader(tokenized_dataset, batch_size=2, shuffle=True, collate_fn=default_data_collator)\n",
    "optimizer = AdamW(model.parameters(), lr=1e-4)\n",
    "\n",
    "def add_labels(example):\n",
    "    input_ids = example[\"input_ids\"]\n",
    "    labels = input_ids.copy()\n",
    "\n",
    "    # \"SVG:\" ÌÜ†ÌÅ∞ÏùÑ Í∏∞Ï§ÄÏúºÎ°ú ÏãúÏûëÏ†ê Ï∞æÍ∏∞\n",
    "    try:\n",
    "        svg_start = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(\"SVG:\"))[-1]\n",
    "        start_idx = input_ids.index(svg_start)\n",
    "    except ValueError:\n",
    "        start_idx = 0  # fallback: Ï†ÑÏ≤¥ ÏòàÏ∏°\n",
    "\n",
    "    # ÎßàÏä§ÌÇπ Ï≤òÎ¶¨\n",
    "    labels[:start_idx + 1] = [-100] * (start_idx + 1)\n",
    "\n",
    "    example[\"labels\"] = labels\n",
    "    return example\n",
    "\n",
    "tokenized_dataset = tokenized_dataset.map(add_labels)\n",
    "tokenized_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])  # üî• Ïó¨Í∏∞Í∞Ä ÌïµÏã¨\n",
    "\n",
    "\n",
    "for epoch in range(5):\n",
    "    total_loss = 0\n",
    "    for step, batch in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)  # ‚úÖ ÏàòÏ†ïÎêú Î∂ÄÎ∂Ñ\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        if step % 50 == 0:\n",
    "            print(f\"Epoch {epoch+1}, Step {step}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "    print(f\"Epoch {epoch+1} Completed. Avg Loss: {total_loss / len(dataloader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1c663baa-7b02-44c2-b29a-ffdc571d8331",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cairosvg\n",
    "from PIL import Image\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1a371021-dffc-4054-8217-7e7a8d7ed429",
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "clip_model, preprocess = clip.load(\"ViT-B/32\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "acc2b3e3-7064-4d89-827e-79b42255f2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_svg(caption):\n",
    "    model.eval()\n",
    "\n",
    "    prompt = f\"\"\"Given the <CAPTION>, generate the corresponding svg code.\n",
    "Return a response WITH SVG CODE that appropriately alignes with the request, IT SHOULD BE VERY ELABORATE.\n",
    "Before answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\n",
    "\n",
    "Please ensure that the generated SVG code is well-formed, valid, and strictly adheres to these constraints.\n",
    "Focus on a clear and concise representation of the input description within the given limitations. Always give the complete SVG code with nothing omitted. Never use an ellipsis.\n",
    "\n",
    "<constraints>\n",
    "* **Allowed Elements:** `svg`, `path`, `circle`, `rect`, `ellipse`, `line`, `polyline`, `polygon`, `g`, `linearGradient`, `radialGradient`, `stop`, `defs`\n",
    "* **Allowed Attributes:** `viewBox`, `width`, `height`, `fill`, `stroke`, `stroke-width`, `d`, `cx`, `cy`, `r`, `x`, `y`, `rx`, `ry`, `x1`, `y1`, `x2`, `y2`, `points`, `transform`, `opacity`\n",
    "</constraints>\n",
    "\n",
    "<CAPTION>\n",
    "{caption}\n",
    "</CAPTION>\n",
    "\n",
    "SVG:\"\"\"\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=4096,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.95,\n",
    "            top_k=50\n",
    "        )\n",
    "\n",
    "    decoded = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return decoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "712e2a65-69f2-4c4f-8563-922adb4d1d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def svg_to_image(svg_code):\n",
    "    png_bytes = cairosvg.svg2png(bytestring=svg_code.encode(\"utf-8\"))\n",
    "    image = Image.open(io.BytesIO(png_bytes)).convert(\"RGB\")\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f2c7e0f3-43bd-4613-bc2f-f8a71e0a4ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_clip_score(caption, svg_code):\n",
    "    if not svg_code.strip():\n",
    "        return 0.0\n",
    "\n",
    "    try:\n",
    "        image = svg_to_image(svg_code)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå SVG to image failed: {e}\")\n",
    "        return 0.0\n",
    "\n",
    "    image_input = preprocess(image).unsqueeze(0).to(device)\n",
    "    text_input = clip.tokenize([caption]).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        image_features = clip_model.encode_image(image_input)\n",
    "        text_features = clip_model.encode_text(text_input)\n",
    "\n",
    "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        similarity = (image_features @ text_features.T).item()\n",
    "        return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "afbd242e-0ef0-4e0b-91ce-e81a44045ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_svg_from_output(output: str) -> str:\n",
    "    # \"SVG:\" Ïù¥ÌõÑ Î∂ÄÎ∂ÑÏùÑ Ï∂îÏ∂ú\n",
    "    if \"SVG:\" in output:\n",
    "        output = output.split(\"SVG:\")[-1].strip()\n",
    "    # ÌòπÏãú Ïù¥ÏÉÅÌïú HTML/ÌÖçÏä§Ìä∏Í∞Ä Íª¥ÎèÑ <svgÎ∂ÄÌÑ∞ ÏûêÎ¶Ñ\n",
    "    svg_start = output.find(\"<svg\")\n",
    "    if svg_start != -1:\n",
    "        return output[svg_start:].strip()\n",
    "    return \"\"  # fallback\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "437a7a1d-44eb-43db-8263-cec4af6e5ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_svg = generate_svg(\"A black and white image of a crescent moon.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "00fe6e93-fd11-4229-85a5-da06344f0502",
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_svg = extract_svg_from_output(generated_svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3e0eb8e3-e3a1-4d48-9efa-577d0e4f844a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\" fill=\"none\" stroke=\"black\" stroke-width=\"1\" class=\"svg-image\">\n",
      "<circle cx=\"12\" cy=\"12\" r=\"10\" fill=\"#fff\" class=\"moon\" />\n",
      "<path d=\"M14 6.983L12 10.546L10 6.983L12 10.546L14 6.983\" stroke=\"#fff\" stroke-width=\"1.5\" />\n",
      "</svg>\n"
     ]
    }
   ],
   "source": [
    "print(extracted_svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada13b44-1ed8-4f65-b95e-73c83b94a7bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
